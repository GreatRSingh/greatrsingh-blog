---
title: "Adventure into the depths of Machine Learning"
subtitle: "The core of ML is Automatic Differentiation [The Ability to Calculate Gradients over long series of Operations], in a series of Posts i will go in depths of the Intricate maths behind Machine Learning."
date: "2024-4-1"
---

# <p align="center">Chapter 1: Introduction</p>

It is with great pleasure and anticipation that I announce the inauguration of our forthcoming series of Blogs explaining in great details about Optimisation techniques used in Machine Learning.

We will follow the book - **The Elements of Differentiable Programming** by *Blondel, Mathieu* and *Roulet, Vincent*

> Automatic differentiation is like having your computer tell you the gradient of a function without resorting to finite difference approximation or coding an analytic derivative by hand. Itâ€™s a powerful tool used in machine learning, optimization, neural networks, and more.

That quote might feel very overwhelming to you, but it is just describing what *PyTorch*, *TensorFlow* and *Jax* are. They are simply tools to do AutoDiff for us and make our life simpler.

We will follow the books flow in covering all the topics:
1. Fundamentals: Differentiation and Probabilistic Learning
2. Differentiable Programs: Neural networks, Sequence Networks and Control Flows
3. Differentiating through Programs:
4. Smoothing Programs:

At the end of each Article i will give some simple task or problem you can send the solution to my Email: admin@greatrsingh.in

Well we have reached the end of it so here is the task:
1. Explain **Gradient Descent** Algorithm and investigate its history.

----

References and Useful Resources:

1. [The Elements of Differentiable Programming](https://arxiv.org/pdf/2403.14606.pdf) by *Blondel, Mathieu* and *Roulet, Vincent*.
